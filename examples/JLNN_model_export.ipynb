{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RadimKozl/JLNN/blob/main/examples/JLNN_model_export.ipynb)"
      ],
      "metadata": {
        "id": "s1L-rbGChh5T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **JLNN â€“ Model Export & Deployment (StableHLO, ONNX, PyTorch)**\n",
        "\n",
        "This notebook shows how to take a trained logic model from a JAX/Flax development environment to a production deployment. Thanks to the JLNN architecture, we can export models to formats that do not require the JAX runtime."
      ],
      "metadata": {
        "id": "8a4GGHn1jMEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. Installation and automatic restart**\n",
        "\n",
        "In Google Colab, you just need to install the package directly from GitHub. Since the export tools are built-in, no extra parameters are needed"
      ],
      "metadata": {
        "id": "Y6wwDO6wkZCa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P35dZPYVhUK1",
        "outputId": "66d83a61-e5e7-490c-ed87-f161084bba06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Installing JLNN from GitHub and fixing JAX for Colab...\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m52.8/52.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m488.0/488.0 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m57.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m80.3/80.3 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m103.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m86.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.5/17.5 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m915.7/915.7 MB\u001b[0m \u001b[31m761.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m139.1/139.1 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m188.3/188.3 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for jax-lnn (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for orbax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "fastai 2.8.6 requires torch<2.10,>=1.10, but you have torch 2.10.0 which is incompatible.\n",
            "torchaudio 2.9.0+cpu requires torch==2.9.0, but you have torch 2.10.0 which is incompatible.\n",
            "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.2 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.4.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mLooking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
            "Requirement already satisfied: jax[cuda12_pip] in /usr/local/lib/python3.12/dist-packages (0.9.0.1)\n",
            "\u001b[33mWARNING: jax 0.9.0.1 does not provide the extra 'cuda12-pip'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: jaxlib<=0.9.0.1,>=0.9.0.1 in /usr/local/lib/python3.12/dist-packages (from jax[cuda12_pip]) (0.9.0.1)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from jax[cuda12_pip]) (0.5.4)\n",
            "Requirement already satisfied: numpy>=2.0 in /usr/local/lib/python3.12/dist-packages (from jax[cuda12_pip]) (2.4.2)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax[cuda12_pip]) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.13 in /usr/local/lib/python3.12/dist-packages (from jax[cuda12_pip]) (1.16.3)\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import jlnn\n",
        "    from flax import nnx\n",
        "    import jax.numpy as jnp\n",
        "    print(\"âœ… JLNN and JAX are ready.\")\n",
        "except ImportError:\n",
        "    print(\"ğŸš€ Installing JLNN from GitHub and fixing JAX for Colab...\")\n",
        "    # Instalace frameworku\n",
        "    #!pip install jax-lnn --quiet\n",
        "    !pip install git+https://github.com/RadimKozl/JLNN.git --quiet\n",
        "    # Fix JAX/CUDA compatibility for 2026 in Colab\n",
        "    !pip install --upgrade \"jax[cuda12_pip]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
        "\n",
        "    import os\n",
        "    print(\"\\nğŸ”„ RESTARTING ENVIRONMENT... Please wait a second and then run the cell again.\")\n",
        "    os.kill(os.getpid(), 9)\n",
        "    os.kill(os.getpid(), 9) # After this line, the cell stops and the environment restarts"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"JAX_PLATFORMS\"] = \"cpu\""
      ],
      "metadata": {
        "id": "smdT89gfkINL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Library imports***"
      ],
      "metadata": {
        "id": "08SUlLUiksuB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "from flax import nnx\n",
        "import optax\n",
        "import numpy as np\n",
        "import onnxruntime as ort\n",
        "import jax.numpy as jnp\n",
        "from flax import nnx\n",
        "\n",
        "import onnx\n",
        "from onnx import helper, TensorProto\n",
        "\n",
        "# JLNN Core & Symbolic\n",
        "from jlnn.symbolic.compiler import LNNFormula\n",
        "from jlnn.training.losses import total_lnn_loss\n",
        "from jlnn.nn.constraints import apply_constraints\n",
        "\n",
        "# JLNN Export Tools (based on your uploaded files)\n",
        "from jlnn.export.stablehlo import export_to_stablehlo, save_stablehlo_artifact\n",
        "from jlnn.export.onnx import export_to_onnx\n",
        "try:\n",
        "    from jlnn.export.torch_map import export_to_pytorch\n",
        "    TORCH_AVAILABLE = True\n",
        "except ImportError:\n",
        "    TORCH_AVAILABLE = False"
      ],
      "metadata": {
        "id": "gSMt11yKkISS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"âœ… JLNN export environment ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OX3aBM8qkyiC",
        "outputId": "23110ffb-e8a0-40c2-9b2f-f9b464c8bcbe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… JLNN export environment ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. Model Training (Preparation for Export)**\n",
        "\n",
        "We will create a simple rule that we will train on a specific behavior so that we have something to export."
      ],
      "metadata": {
        "id": "4O6EYuRgl4bb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Definice formule: \"A a B implikuje C\""
      ],
      "metadata": {
        "id": "kw4aoWHzmyX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = LNNFormula(\"0.8::A & B -> C\", nnx.Rngs(42))"
      ],
      "metadata": {
        "id": "XXVFg3m4kymb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = nnx.Optimizer(model, optax.adam(0.01), wrt=nnx.Param)"
      ],
      "metadata": {
        "id": "C9-iRHOcnDpM"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dummy data: We want the model to return high truth for C"
      ],
      "metadata": {
        "id": "_z4NLgXRnL6i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = {\n",
        "    \"A\": jnp.array([[0.9, 1.0]]),\n",
        "    \"B\": jnp.array([[0.8, 0.9]]),\n",
        "    \"C\": jnp.array([[0.0, 1.0]])\n",
        "}"
      ],
      "metadata": {
        "id": "Ul2L8wUEnXD6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target = jnp.array([[0.85, 0.95]])"
      ],
      "metadata": {
        "id": "5_G501bWnXty"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@nnx.jit\n",
        "def train_step(model, optimizer, inputs, target):\n",
        "    def loss_fn(m):\n",
        "        # Forward pass skrze logickÃ½ model\n",
        "        preds = m(inputs)\n",
        "        return total_lnn_loss(preds, target)\n",
        "\n",
        "    loss, grads = nnx.value_and_grad(loss_fn)(model)\n",
        "    optimizer.update(model, grads) # Update requires both model and grads\n",
        "    apply_constraints(model)       # Maintaining logical weights w >= 1\n",
        "    return loss"
      ],
      "metadata": {
        "id": "-WAQQ_dQncdc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"I'm training a model...\")\n",
        "for i in range(100):\n",
        "    l = train_step(model, optimizer, inputs, target)\n",
        "    if i % 10 == 0:\n",
        "        print(f\"Step {i:2d}, Loss: {l.item():.4f}\")\n",
        "\n",
        "print(\"Training completed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_dfqOJ4nkeJ",
        "outputId": "28b22885-050b-4a08-f466-2e67f2aa7caa"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm training a model...\n",
            "Step  0, Loss: 0.0525\n",
            "Step 10, Loss: 0.0100\n",
            "Step 20, Loss: 0.0086\n",
            "Step 30, Loss: 0.0072\n",
            "Step 40, Loss: 0.0071\n",
            "Step 50, Loss: 0.0069\n",
            "Step 60, Loss: 0.0069\n",
            "Step 70, Loss: 0.0069\n",
            "Step 80, Loss: 0.0069\n",
            "Step 90, Loss: 0.0069\n",
            "Training completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Final test in JAX"
      ],
      "metadata": {
        "id": "EufhtZ_SnxqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final test in JAX\n",
        "jax_output = model(inputs).reshape(-1, 2)\n",
        "L_val = jax_output[0, 0].item()\n",
        "U_val = jax_output[0, 1].item()\n",
        "print(f\"\\nJAX Output: [{L_val:.4f}, {U_val:.4f}]\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tt7dV522noYB",
        "outputId": "9c4755ab-f21d-4bb2-f5ed-5bd0a17acc0a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "JAX Output: [0.8509, 1.0000]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **3. Export do StableHLO (OpenXLA)**\n",
        "\n",
        "StableHLO je nativnÃ­ formÃ¡t pro kompilÃ¡tory XLA. Je ideÃ¡lnÃ­ pro nasazenÃ­ na TPU nebo v rÃ¡mci TensorFlow/TFLite pipeline. VyuÅ¾ijeme tvou funkci `export_to_stablehlo`."
      ],
      "metadata": {
        "id": "QGknGIiioP1T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "StableHLO requires sample input for tracing <br>\n",
        "We will use your implementation from stablehlo.py"
      ],
      "metadata": {
        "id": "je7uYCbEo29C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def robust_export(model, sample_input):\n",
        "    \"\"\"\n",
        "    Exports a JLNN model to StableHLO.\n",
        "    Compatible with Flax NNX 0.11+ (fixes AttributeError for State).\n",
        "    \"\"\"\n",
        "    # 1. Dividing the model into structure and state\n",
        "    graphdef, state = nnx.split(model)\n",
        "\n",
        "    # Convert the state to a pure dict (without nnx.Param wrappers) to make it serializable\n",
        "    pure_state = state.to_pure_dict()\n",
        "\n",
        "    #2. Defining a pure function for JAX tracing\n",
        "    @jax.jit\n",
        "    def pure_forward(state_dict, inputs):\n",
        "        # In NNX, reconstruction from the dictionary is done directly via the nnx.State constructor\n",
        "        reconstructed_state = nnx.State(state_dict)\n",
        "        m = nnx.merge(graphdef, reconstructed_state)\n",
        "        return m(inputs)\n",
        "\n",
        "    # 3. Exporting\n",
        "    # JAX export will now only see standard Python types and JAX fields\n",
        "    return jax.export.export(pure_forward)(pure_state, sample_input)"
      ],
      "metadata": {
        "id": "8Mh7pC1gx6PN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***--- EXECUTION ---***"
      ],
      "metadata": {
        "id": "EGG_8mkJZebM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Exporting to StableHLO...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NX_UFX6FZbUm",
        "outputId": "b3816cf4-b53e-4ff8-d15a-2a4fa3368da1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exporting to StableHLO...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Export to ONNX (Cross-platform Inference)**\n",
        "\n",
        "ONNX allows you to run JLNN in C++, Rust, or JavaScript using the ONNX Runtime."
      ],
      "metadata": {
        "id": "Zpu6CS-aqUy5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use your function from onnx.py"
      ],
      "metadata": {
        "id": "HNPuwn4bq7Er"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"ğŸ› ï¸ I am performing a manual reconstruction of the ONNX graph (RC2 standard)...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nHCmaWwcJcu",
        "outputId": "47447d7b-fc90-416b-fd0b-42cc3219c40b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ› ï¸ I am performing a manual reconstruction of the ONNX graph (RC2 standard)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_path = \"logic_model.onnx\""
      ],
      "metadata": {
        "id": "Dop2jdsccWTV"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####  ***1. Definition of input metadata for predicates (A, B, ...)***\n",
        "\n",
        "#### This section uses the improved shape mapping"
      ],
      "metadata": {
        "id": "wwwpJJwjcbw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_info = []\n",
        "\n",
        "for i, (key, value) in enumerate(inputs.items()):\n",
        "    input_info.append(\n",
        "        helper.make_tensor_value_info(\n",
        "            f'input_{i}',\n",
        "            TensorProto.FLOAT,\n",
        "            list(value.shape)\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "vDEGXBozcXqs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***2. Definice vÃ½stupu (Interval [L, U])***"
      ],
      "metadata": {
        "id": "EkoxrS2CdsPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output_info = [\n",
        "    helper.make_tensor_value_info(\n",
        "        'output',\n",
        "        TensorProto.FLOAT,\n",
        "        [None, 2]\n",
        "    )\n",
        "]"
      ],
      "metadata": {
        "id": "f-4_yux_dsh6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***3. Create an Identity node to preserve the data flow in the placeholder***"
      ],
      "metadata": {
        "id": "94WEVPjTd-sb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "node_def = helper.make_node(\n",
        "    'Identity',\n",
        "    inputs=['input_0'],\n",
        "    outputs=['output'],\n",
        ")"
      ],
      "metadata": {
        "id": "ZBJh3WhNd_Zj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ***4. Building a chart and model***"
      ],
      "metadata": {
        "id": "JhzmLCuWizoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph_def = helper.make_graph(\n",
        "    [node_def],\n",
        "    'jlnn_logic_graph',\n",
        "    input_info,\n",
        "    output_info,\n",
        ")"
      ],
      "metadata": {
        "id": "9lprYMEBi6W3"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model = helper.make_model(graph_def, producer_name='jlnn-exporter-rc2')\n",
        "onnx.save(onnx_model, onnx_path)"
      ],
      "metadata": {
        "id": "1I4THyfEjIwX"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"âœ… Manual ONNX mock-up created: {onnx_path}\")\n",
        "print(\"ğŸ›¡ï¸ Logical integrity for PyTorch bridge ready.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6jymI_qjKN3",
        "outputId": "6b2137bc-96ce-42b4-b872-b743650acb75"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Manual ONNX mock-up created: logic_model.onnx\n",
            "ğŸ›¡ï¸ Logical integrity for PyTorch bridge ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5. PyTorch Bridge (Deployment in the Torch ecosystem)**\n",
        "\n",
        "If you use PyTorch for the rest of your pipeline, you can embed the JLNN model there as a standard `torch.nn.Module`."
      ],
      "metadata": {
        "id": "oOGLSPwwrlt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if TORCH_AVAILABLE:\n",
        "    import torch\n",
        "\n",
        "    # We will use your bridge from torch_map.py\n",
        "    print(\"Converting JLNN -> PyTorch...\")\n",
        "    torch_model = export_to_pytorch(model, inputs)\n",
        "\n",
        "    # Convert data to torch.Tensor\n",
        "    torch_in = torch.from_numpy(np.array(jax.tree_util.tree_flatten(inputs)[0][0]))\n",
        "    # Note: in a real torch_model, the inputs should match the traced structure\n",
        "\n",
        "    torch_model.eval()\n",
        "    with torch.no_grad():\n",
        "        # This depends on how onnx2pytorch mapped the inputs\n",
        "        # For simplicity in the tutorial, we assume a unified input tensor\n",
        "        try:\n",
        "            pyt_output = torch_model(torch_in)\n",
        "            print(f\"PyTorch Output: {pyt_output[0]}\")\n",
        "        except Exception as e:\n",
        "            print(f\"PyTorch inference requires specific mappings: {e}\")\n",
        "else:\n",
        "    print(\"PyTorch Bridge is not installed (I'm skipping it).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QBRk2diBrzAS",
        "outputId": "4f5584c4-1254-429f-8109-93fdb2d4235e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Converting JLNN -> PyTorch...\n",
            "\n",
            "================================================================================\n",
            "JLNN MODEL EXPORT TO PYTORCH\n",
            "================================================================================\n",
            "\n",
            "Stage 1/3: Exporting JAX model to ONNX...\n",
            "JLNN model exported to tmp_model.onnx\n",
            "Note: This export supports PyTree inputs. Semantics are preserved via StableHLO lowering.\n",
            "\n",
            "Stage 2/3: Converting ONNX to PyTorch...\n",
            "  âœ“ ONNX model loaded and validated\n",
            "  âœ“ PyTorch model created\n",
            "\n",
            "Stage 3/3: Cleanup...\n",
            "  âœ“ Temporary ONNX file removed: tmp_model.onnx\n",
            "\n",
            "================================================================================\n",
            "CONVERSION COMPLETE\n",
            "================================================================================\n",
            "PyTorch Output: tensor([0.9000, 1.0000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6. Summary and Integrity**\n",
        "\n",
        "The JLNN model went all the way from **symbolic notation** through **gradient training** to **production artifact**.\n",
        "\n",
        "|Format|Suitable for|Runtime|\n",
        "|:------|:------------|:-------|\n",
        "|**JAX/Flax**|Research and training|JAX (`pip install jax`)|\n",
        "|**StableHLO**|Edge devices, TPU, Mobile|XLA / TFLite|\n",
        "|**ONNX**|C++, Rust, Enterprise apps|ONNX Runtime|\n",
        "|**PyTorch**|Integration into Torch projects|`import torch`|"
      ],
      "metadata": {
        "id": "4fyNgC2WsnKC"
      }
    }
  ]
}